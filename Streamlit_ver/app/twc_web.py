import os
import tempfile
import json
import pandas as pd
import re
import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.output_parsers import StrOutputParser

from config import settings
from twc_rag import get_chain

os.environ['CURL_CA_BUNDLE'] = ''

retriever_config = settings.RAG_FILES
model_config = settings.MODEL_CONFIG.root
retrieval_doc_visiable = settings.RETRIEVAL_DOC_VISIABLE
is_local = True
for language, file_path in retriever_config.items():
    with open(file_path, 'r', encoding='utf-8') as file:
        faq_json_array = json.load(file)

def get_messages(msgs, max_pairs):
    """
    è™•ç†å‚³å…¥çµ¦ chain çš„ chat_hostory
    """
    
    if max_pairs == 0:
        return []
    
    # ç¡®å®š msgs çš„é•¿åº¦
    n = len(msgs)

    # è®¡ç®—å¼€å§‹ç´¢å¼•ï¼Œç¡®ä¿æˆ‘ä»¬è‡³å°‘æœ‰ required_min_length ä¸ªå…ƒç´ ï¼Œå¦åˆ™ä» 1 å¼€å§‹è·³è¿‡ç¬¬ä¸€ä¸ª ai
    start_index = 1

    # å¦‚æœåˆ—è¡¨ä¸­æœ‰è¶³å¤Ÿçš„å…ƒç´ æ¥å½¢æˆè‡³å°‘ä¸€å¯¹ (human, ai)
    if n > 1:
        # å–ç›¸åº”çš„å…ƒç´ è¿›è¡Œå¤„ç†
        last_elements = msgs[start_index:]

        # ä½¿ç”¨ zip ç”Ÿæˆ (human, ai) å…ƒç»„ï¼Œç¡®ä¿åªå–æœ€å max_pairs å¯¹
        # è¿™é‡Œä¿®æ”¹äº† zip çš„é¡ºåºï¼Œå°† ai, human æ”¹ä¸º human, ai
        result = [(human.content, ai.content) for human, ai in zip(last_elements[::2], last_elements[1::2])]
        result = result[-max_pairs:]
    else:
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å…ƒç´ è¿›è¡Œé…å¯¹ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        result = []

    # print(f"msgs: {msgs}")
    # print(f"result: {result}")
    # æ˜¾ç¤ºç»“æœæˆ–è¿›è¡Œå…¶ä»–æ“ä½œ
    return result

# å®šç¾©é é¢åŸºæœ¬è³‡è¨Š
# ç€è¦½å™¨ä¸Šçš„ page title å’Œ åœ–ç¤º
st.set_page_config(page_title="å°æ°´æ™ºæ…§å®¢æœDemo", page_icon="favicon.ico")
# ç•¶å‰é é¢çš„ title
st.title("å°æ°´æ™ºæ…§å®¢æœDemo")

# sidebar ç›¸é—œç‰©ä»¶
with st.sidebar:  
    def on_model_change():
        msgs.clear()
        model = st.session_state.select_llm_model
        text = f"å·²åˆ‡æ›åˆ° {model} æ¨¡å‹"
        st.toast(text)
    
    select_llm_models = model_config.keys()
    
    select_llm_model = st.selectbox("è«‹é¸æ“‡æ¨¡å‹ä¾†æºï¼š",
        select_llm_models,
        index=0,
        on_change=on_model_change,
        key="select_llm_model",   # å°‡ä¿®æ”¹çµæœä¿å­˜æ–¼ session_state
    )

llm_model_switch = { key: value.asInstance() for key, value in model_config.items() }
llm_model = llm_model_switch.get(select_llm_model)

if llm_model in ["gpt-4o","gpt-3.5-turbo-0125"]:
    is_local = False
chain = get_chain(llm_model, is_local=is_local)

# Setup memory for contextual conversation
msgs = StreamlitChatMessageHistory()
  
if len(msgs.messages) == 0 or st.sidebar.button("æ¸…é™¤æ­·å²è¨Šæ¯"):
    msgs.clear()
    msgs.add_ai_message("æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨?")

avatars = {"human": "user", "ai": "ğŸ¤–"}
for msg in msgs.messages:
    st.chat_message(avatars[msg.type]).write(msg.content)

# st.chat_input æ˜¯ chatbot å°è©±ç‰©ä»¶
if user_query := st.chat_input(placeholder="è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    st.chat_message("user").write(user_query)

    with st.chat_message("ğŸ¤–"):        
        full_response = ""
        new_response = ""
        docs = []
        if retrieval_doc_visiable:
            retrieval_handler = st.container()
        response_handler = st.empty()
        # print(msgs.messages)

        no_reply = False
        for response in chain.stream(
            {
                "question": user_query,
                "chat_history": get_messages(msgs.messages, 0)
            }
        ):
            print(f"---------------------------------------------{response}")

            if "response" in response:
                new_response+= response['response']
            elif "context" in response and retrieval_doc_visiable:
                try:
                    docs = response["context"]
                    json_objects = docs.strip().split('\n')
                    dict_list = [json.loads(obj) for obj in json_objects]
                    retrieval_handler_status = retrieval_handler.status("**è³‡æ–™æª¢ç´¢**")
                    retrieval_handler_status.write(f"**Question:** {user_query}")
                    retrieval_handler_status.update(label=f"**è³‡æ–™æª¢ç´¢:** {user_query}")
                    
                    for items in dict_list:
                        retrieval_handler_status.markdown(items["question"])
                        retrieval_handler_status.markdown(items["answer"])
                    retrieval_handler_status.update(state="complete")
                except:
                    pass
        
        try:
            new_response = new_response.strip('{}')
            new_response = new_response.split(',',1)
            id , response = new_response[0], new_response[1]
            id_value = id.split(':',1)[1]
            response_value = response.split(':',1)[1]
            full_response += response_value+"\n"
            full_response += "\n"+id_value if id_value else None
        except:
            full_response = 'ä¸å¥½æ„æ€ï¼Œè«‹æ‚¨åœ¨è¼¸å…¥ä¸€æ¬¡å•é¡Œ'
        
        response_handler.markdown(full_response)

    msgs.add_user_message(user_query)
    msgs.add_ai_message(full_response)