import os
import json
import tempfile
import streamlit as st
from code_editor import code_editor
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage, ChatMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain.memory import ConversationBufferWindowMemory

from operator import itemgetter

from chat_prompt import chat_prompt

from base_rag_chain import get_rag_chain
from conversation_rag_chain import get_conversation_rag_chain
from chat_chain import get_chat_chain
from self_rag_graph import get_self_rag_graph
from kgi_rag import get_rag_chain as get_kgi_rag
from kgi_rag_v1c import get_rag_chain as get_kgi_rag_v1c

from retriever.itri_retriever import ITRIRetriever

from config import settings
from itri_api import ITRI_API
from prompts import prompts

import langchain
langchain.debug = settings.DEBUG


model_config = settings.MODEL_CONFIG.root
# retrieval_config = settings.RETRIEVER_CONFIG.root
retrieval_doc_visiable = settings.RETRIEVAL_DOC_VISIABLE
rag_redouce_below_limit_token = settings.RAG_REDOUCE_BELOW_LIMIT_TOKEN

default_chat_mode = settings.DEFAULT_CHAT_MODE
default_chat_history_windows = settings.DEFAULT_CHAT_HISTORY_WINDOWS
# chat_history_windows = settings.CHAT_HISTORY_WINDOWS


def get_messages(msgs, max_pairs):
    """
    è™•ç†å‚³å…¥çµ¦ chain çš„ chat_hostory
    """
    
    if max_pairs == 0:
        return []
    
    # ç¡®å®š msgs çš„é•¿åº¦
    n = len(msgs)

    # è®¡ç®—å¼€å§‹ç´¢å¼•ï¼Œç¡®ä¿æˆ‘ä»¬è‡³å°‘æœ‰ required_min_length ä¸ªå…ƒç´ ï¼Œå¦åˆ™ä» 1 å¼€å§‹è·³è¿‡ç¬¬ä¸€ä¸ª ai
    start_index = 1

    # å¦‚æœåˆ—è¡¨ä¸­æœ‰è¶³å¤Ÿçš„å…ƒç´ æ¥å½¢æˆè‡³å°‘ä¸€å¯¹ (human, ai)
    if n > 1:
        # å–ç›¸åº”çš„å…ƒç´ è¿›è¡Œå¤„ç†
        last_elements = msgs[start_index:]

        # ä½¿ç”¨ zip ç”Ÿæˆ (human, ai) å…ƒç»„ï¼Œç¡®ä¿åªå–æœ€å max_pairs å¯¹
        # è¿™é‡Œä¿®æ”¹äº† zip çš„é¡ºåºï¼Œå°† ai, human æ”¹ä¸º human, ai
        result = [(human.content, ai.content) for human, ai in zip(last_elements[::2], last_elements[1::2])]
        result = result[-max_pairs:]
    else:
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å…ƒç´ è¿›è¡Œé…å¯¹ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        result = []

    # print(f"msgs: {msgs}")
    # print(f"result: {result}")
    # æ˜¾ç¤ºç»“æœæˆ–è¿›è¡Œå…¶ä»–æ“ä½œ
    return result

def get_retrieval_config(api: ITRI_API, retrieval_top_n=1):
    """
    å–å¾—ç•¶å‰æ‰€æœ‰æª¢ç´¢ä¾†æº (çŸ¥è­˜åº«)
    """
    data_type_list = api.get_data_type_list()
    retrieval_config = {}
    for data_type in data_type_list:
        data_type_name = data_type["data_type_name"]
        data_type_id = data_type["data_type_id"]
        retrieval_config[data_type_name] = ITRIRetriever(
            url=f"{settings.ITRI_KM_CONFIG['admin_api_base_url']}/v1/knowledge/search",
            data_type_ids=[data_type_id],
            threshold=0.80,
            limit=retrieval_top_n,
            auth=True,
            api=api
        )     
    return retrieval_config


def get_chains(chat_mode, llm_model, retriever, prompts, rag_document_tokens_limit=rag_redouce_below_limit_token):
    """
    åŸºæ–¼é¸å–çš„ chat_mode ä¾†è®€å–æœ€æ–° prompts ä¸¦ç”¢ç”Ÿæ”¾å…¥åˆ°chat_modeå°æ‡‰çš„ chain
    """
    def _build_message_prompt(system_message, user_prompt):
        messages = []
        if system_message:
            messages.append(SystemMessage(content=system_message))
        
        messages.append(HumanMessagePromptTemplate.from_template(user_prompt))

        full_prompts = ChatPromptTemplate.from_messages(messages)
        
        return full_prompts
    
    if chat_mode == "Chat":
        chat_chain = get_chat_chain(llm_model, chat_prompt)
        return chat_chain
    elif chat_mode == "RAG":
        rag_prompt = _build_message_prompt("", prompts['RAG']['generate'])
        return get_rag_chain(llm_model, rag_prompt, retriever)
    elif chat_mode == "RAG_Conversation":
        return get_conversation_rag_chain(llm_model, retriever)
    elif chat_mode == "Self-RAG":
        return get_self_rag_graph(llm_model, retriever)
    elif chat_mode =="KGI POC":
        kgi_prompt = _build_message_prompt(prompts['KGI POC']['system'], prompts['KGI POC']['generate'])
        return get_kgi_rag(llm_model, retriever, kgi_prompt, max_input_tokens_limit=rag_document_tokens_limit)
    elif chat_mode =="KGI POC v1c":
        kgi_prompt = _build_message_prompt(prompts['KGI POC v1c']['system'], prompts['KGI POC v1c']['generate'])
        return get_kgi_rag_v1c(llm_model, retriever, kgi_prompt)
    elif chat_mode =="Prompt Test":
        prompte_test_prompt = ChatPromptTemplate.from_template(prompts['Prompt Test']['generate'])
        return get_chat_chain(llm_model, prompte_test_prompt)
    elif chat_mode =="Perplexity Test":
        prompte_test_prompt = _build_message_prompt(prompts['Perplexity Test']['system'], prompts['Perplexity Test']['generate'])
        return get_rag_chain(llm_model, prompte_test_prompt,retriever)
    else:
        st.info("ä¸æ”¯æ´çš„æ¨¡å¼")
        st.stop()
    



def chat_web(api: ITRI_API):
    title = f"""å·¥ç ”é™¢è³‡é€šæ‰€â°å ´åŸŸè«®è©¢å±•ç¤º"""
    st.title(title)
    description = f""""""
    st.caption(description, unsafe_allow_html=True)
    
    """
    ä»¥ä¸‹æ˜¯è™•ç†ç•¶å‰ session ä½¿ç”¨çš„ prompts
    """
    if "current_prompts" in st.session_state :
        current_prompts = st.session_state["current_prompts"]
    else:
        current_prompts = prompts
    
    current_prompts = prompts
    with st.expander("Prompt ä¿®æ”¹"):
        st.caption("<ul><li>æ–¼ä¿®æ”¹ prompt å€å¡Šä¿®æ”¹ï¼Œä¸¦é»æ“Š save å¾Œã€‚</li><li>å¯æ–¼ç›®å‰å¥—ç”¨çš„ promptå€å¡ŠæŸ¥çœ‹ç•¶å‰ç”Ÿæ•ˆçš„ prompt</li><li>é é¢é‡æ–°æ•´ç†æˆ–åˆ‡æ›è‡³çŸ¥è­˜åº«éƒ½éœ€è¦é‡æ–°è¨­å®š</li></ul>", unsafe_allow_html=True)
        
        st.write("åŸå§‹ prompt:")
        st.json(prompts, expanded=False)
        st.write("ä¿®æ”¹ promt (è«‹é»æ“Šæ—é‚Šçš„ save åœ–ç¤ºä¾†ä¿å­˜):")
        response_dict = code_editor(
            json.dumps(
                current_prompts, ensure_ascii=False, indent=4
            ), lang="json", height=[10, 20], buttons=[
                {
                    "name": "Save",
                    "feather": "Save",
                    "hasText": True,
                    "commands": [
                    "save-state",
                    [
                        "response",
                        "saved"
                    ]
                    ],
                    "response": "saved",
                    "style": {
                    "bottom": "calc(50% - 4.25rem)",
                    "right": "0.4rem"
                    }
                }
        ])
        if response_dict["text"]:
            current_prompts = json.loads(response_dict["text"])
            st.session_state["current_prompts"] = current_prompts
        st.write("ç›®å‰å¥—ç”¨çš„ prompt:")
        st.json(current_prompts, expanded=False)
    """
    ä»¥ä¸Šæ˜¯è™•ç†ç•¶å‰ session ä½¿ç”¨çš„ prompts è¨­å®š
    """

    retrieval_config = get_retrieval_config(api)

    with st.sidebar:
        def on_chat_mode_change():
            msgs.clear()
            chat_mode = st.session_state.chat_mode
            text = f"å·²åˆ‡æ›åˆ° {chat_mode} æ¨¡å¼"
            st.toast(text)
        
        chat_modes = [
            "RAG",
            "RAG_Conversation",
            "KGI POC",
            "KGI POC v1c",
            "Self-RAG",
            "Chat",
            "Prompt Test",
            "Perplexity Test",
        ]
        
        chat_mode = st.selectbox("è«‹é¸æ“‡å°è©±æ¨¡å¼",
            chat_modes,
            index=default_chat_mode,
            on_change=on_chat_mode_change,
            key="chat_mode",
        )
        
        def on_retrieval_change():
            msgs.clear()
            retrieval = st.session_state.retrieval_source
            text = f"å·²åˆ‡æ›åˆ° {retrieval} çŸ¥è­˜åº«"
            st.toast(text)
        
        retrieval_sources = retrieval_config.keys()
        
        retrieval_source = st.selectbox("è«‹é¸æ“‡ RAG çš„çŸ¥è­˜åº«ï¼š",
            retrieval_sources,
            index=0,
            on_change=on_retrieval_change,
            key="retrieval_source",
            disabled=(chat_mode not in ["RAG", "RAG_Conversation", "KGI POC", "Self-RAG", "Perplexity Test"])
        )
        
        chat_history_windows = st.number_input(
            "æ­·å²å°è©±è¼ªæ•¸ï¼š", 
            0, 
            4, 
            default_chat_history_windows,
            disabled=(chat_mode in ["RAG"]))
        
        retrieval_top_n = st.number_input(
            "æª¢ç´¢å›å‚³æ–‡ä»¶æ•¸", 
            1, 
            4, 
            4,
            disabled=(chat_mode not in ["RAG", "RAG_Conversation", "KGI POC", "Self-RAG", "Perplexity Test"])
        )
        
        rag_document_tokens_limit = st.slider("RAG æ–‡ä»¶çš„ Tokens ä¸Šé™", 2048, 8192, rag_redouce_below_limit_token, 128)
        
        def on_model_change():
            msgs.clear()
            model = st.session_state.select_llm_model
            text = f"å·²åˆ‡æ›åˆ° {model} æ¨¡å‹"
            st.toast(text)
        
        select_llm_models = model_config.keys()
        
        select_llm_model = st.selectbox("è«‹é¸æ“‡æ¨¡å‹ä¾†æºï¼š",
            select_llm_models,
            index=0,
            on_change=on_model_change,
            key="select_llm_model",
        )
        
        temperature = st.slider("Temperatureï¼š", 0.0, 0.7, 0.0, 0.05)
        

    llm_model = model_config.get(select_llm_model).asInstance()
    llm_model.temperature = temperature
    retriever = retrieval_config.get(retrieval_source)
    retriever.limit = retrieval_top_n
    

    # Setup memory for contextual conversation
    msgs = StreamlitChatMessageHistory()
    # memory = ConversationBufferMemory(return_messages=True, output_key="answer", input_key="question", chat_memory=msgs)

    if len(msgs.messages) == 0 or st.sidebar.button("æ¸…é™¤æ­·å²è¨Šæ¯"):
        msgs.clear()
        # api_usage_messages.clear()
        msgs.add_ai_message("æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨?")

    avatars = {"human": "user", "ai": "ğŸ¤–"}
    for msg in msgs.messages:
        st.chat_message(avatars[msg.type]).write(msg.content)

    if user_query := st.chat_input(placeholder="è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        st.chat_message("user").write(user_query) 
        

        with st.chat_message("ğŸ¤–"):        
            full_response = ""
            docs = []
            if retrieval_doc_visiable:
                retrieval_handler = st.container()
            response_handler = st.empty()
            chain = get_chains(chat_mode, llm_model, retriever, current_prompts, rag_document_tokens_limit=rag_document_tokens_limit)
            
            if chat_mode in ["Self-RAG"]:
                response = chain.invoke(
                    {
                        "question": user_query,
                        "chat_history": get_messages(msgs.messages, chat_history_windows)
                    }
                )
                full_response = response["generation"]
            else:

                if chat_mode in ["RAG", "RAG_Conversation", "KGI POC", "Perplexity Test"] and retrieval_doc_visiable:
                    retrieval_handler_status = retrieval_handler.status("**è³‡æ–™æª¢ç´¢**")
                
                
                # print(msgs.messages)
                for response in chain.stream(
                    {
                        "question": user_query,
                        "chat_history": get_messages(msgs.messages, chat_history_windows)
                    }
                ):
                    # print(f"---------------------------------------------{response}")
                    if "answer" in response:
                        full_response += response["answer"]
                        response_handler.markdown(full_response + "â–Œ")
                    elif "docs" in response and chat_mode in ["RAG", "RAG_Conversation", "KGI POC", "Perplexity Test"] and retrieval_doc_visiable:
                        docs = response["docs"]
                        retrieval_handler_status.write(f"**Question:** {user_query}")
                        retrieval_handler_status.update(label=f"**è³‡æ–™æª¢ç´¢:** {user_query}")
                        
                        for idx, doc in enumerate(docs):
                            source = os.path.basename(doc.metadata["source"])
                            retrieval_handler_status.write(f"**å‘½ä¸­æ–‡ä»¶ç‰‡æ®µ {idx} æ–¼ {source}**")
                            retrieval_handler_status.markdown(doc.page_content)
                        retrieval_handler_status.update(state="complete")

            response_handler.markdown(full_response)

        msgs.add_user_message(user_query)
        msgs.add_ai_message(full_response)
    

